{"version":3,"file":"tokenizer.js","sourceRoot":"","sources":["../../../src/model/utils/tokenizer.ts"],"names":[],"mappings":"AAAA,OAAO,EAAE,kBAAkB,EAAE,MAAM,UAAU,CAAC;AAK9C,gFAAgF;AAChF,MAAM,cAAc,GAAG,IAAI,GAAG,EAAqB,CAAC;AAEpD,8CAA8C;AAC9C,MAAM,CAAC,MAAM,eAAe,GAAiC,CAC3D,KAAa,EACb,EAAE;IACF,IAAI,cAAc,CAAC,GAAG,CAAC,KAAK,CAAC,EAAE,CAAC;QAC9B,OAAO,cAAc,CAAC,GAAG,CAAC,KAAK,CAAE,CAAC;IACpC,CAAC;SAAM,CAAC;QACN,MAAM,SAAS,GAAG,IAAI,SAAS,CAAC,KAAK,CAAC,CAAC;QACvC,cAAc,CAAC,GAAG,CAAC,KAAK,EAAE,SAAS,CAAC,CAAC;QACrC,OAAO,SAAS,CAAC;IACnB,CAAC;AACH,CAAC,CAAC;AAEF,MAAM,YAAY,GAAG;IACnB,OAAO;IACP,YAAY;IACZ,YAAY;IACZ,YAAY;IACZ,oBAAoB;IACpB,WAAW;IACX,gBAAgB;IAChB,gBAAgB;IAChB,aAAa;IACb,sBAAsB;CACd,CAAC;AAGX,MAAM,SAAS;IACb,KAAK,CAAS;IACd,QAAQ,CAAW;IAEnB,YAAY,KAAa;QACvB,IAAI,CAAC,KAAK,GAAG,KAAK,CAAC;QACnB,IAAI,CAAC;YACH,IAAI,CAAC,QAAQ,GAAG,kBAAkB,CAAC,KAAsB,CAAC,CAAC;QAC7D,CAAC;QAAC,OAAO,CAAC,EAAE,CAAC;YACX,IAAI,CAAC,QAAQ,GAAG,kBAAkB,CAAC,eAAe,CAAC,CAAC;QACtD,CAAC;IACH,CAAC;IAED,4BAA4B;IAC5B,MAAM,CAAC,IAAY;QACjB,OAAO,IAAI,CAAC,QAAQ,CAAC,MAAM,CAAC,IAAI,CAAC,CAAC;IACpC,CAAC;IAED,4BAA4B;IAC5B,MAAM,CAAC,MAA8B;QACnC,MAAM,QAAQ,GAAG,KAAK,CAAC,OAAO,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,IAAI,WAAW,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC;QAC1E,OAAO,IAAI,WAAW,EAAE,CAAC,MAAM,CAAC,IAAI,CAAC,QAAQ,CAAC,MAAM,CAAC,QAAQ,CAAC,CAAC,CAAC;IAClE,CAAC;IAED;;;QAGI;IACJ,WAAW,CAAC,KAA4C;QACtD,IAAI,CAAC,KAAK;YAAE,OAAO,CAAC,CAAC;QACrB,IAAI,OAAO,KAAK,KAAK,QAAQ,EAAE,CAAC;YAC9B,OAAO,IAAI,CAAC,QAAQ,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,MAAM,CAAC;QAC5C,CAAC;aAAM,IAAI,KAAK,CAAC,OAAO,CAAC,KAAK,CAAC,EAAE,CAAC;YAChC,gKAAgK;YAChK,+EAA+E;YAC/E,yDAAyD;YACzD,2EAA2E;YAC3E,MAAM,MAAM,GAAG,IAAI,CAAC,WAAW,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC;YAC5C,MAAM,gBAAgB,GAAG,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;YACxC,IAAI,SAAS,GAAG,CAAC,CAAC;YAElB,KAAK,MAAM,OAAO,IAAI,KAAK,EAAE,CAAC;gBAC5B,SAAS,IAAI,gBAAgB,CAAC;gBAC9B,IAAI,OAAO,CAAC,OAAO,EAAE,CAAC;oBACpB,SAAS,IAAI,IAAI,CAAC,WAAW,CAAC,OAAO,CAAC,OAAO,CAAC,CAAC;gBACjD,CAAC;gBACD,6DAA6D;gBAC7D,IAAI,MAAM,EAAE,CAAC;oBACX,wCAAwC;oBACxC,4EAA4E;oBAC5E,SAAS,IAAI,CAAC,CAAC,CAAC,OAAO;oBACvB,IAAI,OAAO,CAAC,IAAI,EAAE,CAAC;wBACjB,kEAAkE;wBAClE,SAAS,IAAI,IAAI,CAAC,WAAW,CAAC,GAAG,OAAO,CAAC,IAAI,EAAE,CAAC,GAAG,CAAC,CAAC;oBACvD,CAAC;gBACH,CAAC;qBAAM,CAAC;oBACN,sDAAsD;oBACtD,SAAS,IAAI,IAAI,CAAC,WAAW,CAAC,OAAO,CAAC,IAAI,IAAI,OAAO,CAAC,IAAI,CAAC,CAAC;gBAC9D,CAAC;YACH,CAAC;YAED,uCAAuC;YACvC,SAAS,IAAI,CAAC,CAAC;YAEf,OAAO,SAAS,CAAC;QACnB,CAAC;aAAM,CAAC;YACN,OAAO,IAAI,CAAC,WAAW,CAAC,KAAK,CAAC,OAAO,IAAI,EAAE,CAAC,CAAC;QAC/C,CAAC;IACH,CAAC;IAED,kDAAkD;IAClD,QAAQ,CAAC,IAOR;QACC,MAAM,EAAE,IAAI,EAAE,GAAG,EAAE,IAAI,GAAG,OAAO,EAAE,GAAG,IAAI,CAAC;QAC3C,MAAM,MAAM,GAAG,IAAI,CAAC,MAAM,CAAC,IAAI,CAAC,CAAC;QAEjC,IAAI,MAAM,CAAC,MAAM,IAAI,GAAG,EAAE,CAAC;YACzB,OAAO,IAAI,CAAC;QACd,CAAC;QAED,MAAM,eAAe,GACnB,IAAI,KAAK,OAAO;YACd,CAAC,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE,GAAG,CAAC;YACtB,CAAC,CAAC,MAAM,CAAC,KAAK,CAAC,MAAM,CAAC,MAAM,GAAG,GAAG,CAAC,CAAC;QAExC,IAAI,aAAa,GAAG,IAAI,CAAC,MAAM,CAAC,eAAe,CAAC,CAAC;QAEjD,mFAAmF;QACnF,aAAa,GAAG,aAAa,CAAC,OAAO,CAAC,kBAAkB,EAAE,EAAE,CAAC,CAAC;QAE9D,OAAO,aAAa,CAAC;IACvB,CAAC;IAED,mDAAmD;IAC3C,WAAW,CAAC,KAAa;QAC/B,OAAO,YAAY,CAAC,QAAQ,CAAC,KAAY,CAAC,CAAC;IAC7C,CAAC;CACF","sourcesContent":["import { encoding_for_model } from 'tiktoken';\nimport type { Tiktoken, TiktokenModel } from 'tiktoken';\nimport type { ChatMessage } from 'openai-fetch';\nimport type { Model } from '../types.js';\n\n// Store instances of the tokenizer to avoid re-creating them for the same model\nconst tokenizerCache = new Map<string, Tokenizer>();\n\n/** Create a tokenizer for a specific model */\nexport const createTokenizer: (model: string) => Tokenizer = (\n  model: string\n) => {\n  if (tokenizerCache.has(model)) {\n    return tokenizerCache.get(model)!;\n  } else {\n    const tokenizer = new Tokenizer(model);\n    tokenizerCache.set(model, tokenizer);\n    return tokenizer;\n  }\n};\n\nconst GPT_4_MODELS = [\n  'gpt-4',\n  'gpt-4-0314',\n  'gpt-4-0613',\n  'gpt-4-1106',\n  'gpt-4-1106-preview',\n  'gpt-4-32k',\n  'gpt-4-32k-0314',\n  'gpt-4-32k-0613',\n  'gpt-4-turbo',\n  'gpt-4-vision-preview',\n] as const;\ntype Gpt4ModelName = (typeof GPT_4_MODELS)[number];\n\nclass Tokenizer implements Model.ITokenizer {\n  model: string;\n  tiktoken: Tiktoken;\n\n  constructor(model: string) {\n    this.model = model;\n    try {\n      this.tiktoken = encoding_for_model(model as TiktokenModel);\n    } catch (e) {\n      this.tiktoken = encoding_for_model('gpt-3.5-turbo');\n    }\n  }\n\n  /** Encode text to tokens */\n  encode(text: string): Uint32Array {\n    return this.tiktoken.encode(text);\n  }\n\n  /** Decode tokens to text */\n  decode(tokens: number[] | Uint32Array): string {\n    const toDecode = Array.isArray(tokens) ? new Uint32Array(tokens) : tokens;\n    return new TextDecoder().decode(this.tiktoken.decode(toDecode));\n  }\n\n  /**\n   * Count the number of tokens in a string or ChatMessage(s)\n   * A single message is counted as a completion and an array as a prompt\n   **/\n  countTokens(input?: string | ChatMessage | ChatMessage[]): number {\n    if (!input) return 0;\n    if (typeof input === 'string') {\n      return this.tiktoken.encode(input).length;\n    } else if (Array.isArray(input)) {\n      // This is copied from OpenAI's Python implementation: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n      // NOTE: The array version assumes this is for the prompt and adds more tokens.\n      // Use the single ChatMessage for completion token count.\n      // The tokensPerName and tokensPerMessage will need to be updated over time\n      const isGpt4 = this.isGpt4Model(this.model);\n      const tokensPerMessage = isGpt4 ? 3 : 4;\n      let numTokens = 0;\n\n      for (const message of input) {\n        numTokens += tokensPerMessage;\n        if (message.content) {\n          numTokens += this.countTokens(message.content);\n        }\n        // The name+role are handled differently for GPT-3.5 vs GPT-4\n        if (isGpt4) {\n          // For 4, the name and role are included\n          // Details here: https://github.com/openai/openai-python/blob/main/chatml.md\n          numTokens += 1; // role\n          if (message.name) {\n            // No idea why this, but tested with many examples and it works...\n            numTokens += this.countTokens(`${message.name}`) + 1;\n          }\n        } else {\n          // For 3.5, the name replaces the role if it's present\n          numTokens += this.countTokens(message.name || message.role);\n        }\n      }\n\n      // Every reply is primed with assistant\n      numTokens += 3;\n\n      return numTokens;\n    } else {\n      return this.countTokens(input.content || '');\n    }\n  }\n\n  /** Truncate text to a maximum number of tokens */\n  truncate(args: {\n    /** Text to truncate */\n    text: string;\n    /** Maximum number of tokens to keep (inclusive) */\n    max: number;\n    /** Truncate from the start or end of the text */\n    from?: 'start' | 'end';\n  }): string {\n    const { text, max, from = 'start' } = args;\n    const tokens = this.encode(text);\n\n    if (tokens.length <= max) {\n      return text;\n    }\n\n    const truncatedTokens =\n      from === 'start'\n        ? tokens.slice(0, max)\n        : tokens.slice(tokens.length - max);\n\n    let truncatedText = this.decode(truncatedTokens);\n\n    // Handle edge case where the last token is part of a multi-byte character sequence\n    truncatedText = truncatedText.replace(/[\\uDC00-\\uDFFF]$/, '');\n\n    return truncatedText;\n  }\n\n  /** Check if the given model is a GPT-4 variant. */\n  private isGpt4Model(model: string): model is Gpt4ModelName {\n    return GPT_4_MODELS.includes(model as any);\n  }\n}\n"]}