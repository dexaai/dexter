{"version":3,"file":"embedding.js","sourceRoot":"","sources":["../../src/model/embedding.ts"],"names":[],"mappings":"AAAA,OAAO,SAAS,MAAM,YAAY,CAAC;AACnC,OAAO,IAAI,MAAM,OAAO,CAAC;AAIzB,OAAO,EAAE,aAAa,EAAE,MAAM,2BAA2B,CAAC;AAC1D,OAAO,EAAE,kBAAkB,EAAE,MAAM,qBAAqB,CAAC;AACzD,OAAO,EAAE,aAAa,EAAE,MAAM,YAAY,CAAC;AAC3C,OAAO,EAAE,SAAS,EAAE,MAAM,qBAAqB,CAAC;AAiBhD,MAAM,QAAQ,GAAG;IACf,yDAAyD;IACzD,gBAAgB,EAAE,IAAI,GAAG,EAAE,EAAE,WAAW;IACxC,YAAY,EAAE,EAAE;IAChB,iBAAiB,EAAE,KAAK;IACxB,qBAAqB,EAAE,CAAC;IACxB,iBAAiB,EAAE,IAAI;IACvB,KAAK,EAAE,wBAAwB;CACvB,CAAC;AAEX,MAAM,OAAO,cAAe,SAAQ,aAMnC;IACC,SAAS,GAAG,WAAoB,CAAC;IACjC,aAAa,GAAG,QAAiB,CAAC;IAClC,cAAc,CAAe;IAE7B,wFAAwF;IACxF,YAAY,IAAyB;QACnC,IAAI,EAAE,MAAM,EAAE,MAAM,EAAE,GAAG,IAAI,EAAE,GAAG,IAAI,IAAI,EAAE,CAAC;QAC7C,MAAM,GAAG,MAAM,IAAI,kBAAkB,EAAE,CAAC;QACxC,MAAM,GAAG,MAAM,IAAI,EAAE,KAAK,EAAE,QAAQ,CAAC,KAAK,EAAE,CAAC;QAC7C,KAAK,CAAC,EAAE,MAAM,EAAE,MAAM,EAAE,GAAG,IAAI,EAAE,CAAC,CAAC;QACnC,MAAM,QAAQ,GAAG,QAAQ,CAAC,gBAAgB,CAAC;QAC3C,MAAM,KAAK,GACT,IAAI,CAAC,MAAM,CAAC,QAAQ,EAAE,iBAAiB,IAAI,QAAQ,CAAC,iBAAiB,CAAC;QAExE,gCAAgC;QAChC,IAAI,CAAC,cAAc,GAAG,SAAS,CAAC,EAAE,KAAK,EAAE,QAAQ,EAAE,CAAC,CAClD,KAAK,EACH,MAAoD,EACpD,OAAkB,EAClB,EAAE;YACF,MAAM,KAAK,GAAG,IAAI,CAAC,GAAG,EAAE,CAAC;YAEzB,wDAAwD;YACxD,MAAM,QAAQ,GAAG,MAAM,IAAI,CAAC,MAAM,CAAC,gBAAgB,CAAC;gBAClD,KAAK,EAAE,MAAM,CAAC,KAAK;gBACnB,KAAK,EAAE,MAAM,CAAC,KAAK;aACpB,CAAC,CAAC;YAEH,MAAM,OAAO,CAAC,UAAU,CACtB,IAAI,CAAC,MAAM,EAAE,aAAa,EAAE,GAAG,CAAC,CAAC,KAAK,EAAE,EAAE,CACxC,OAAO,CAAC,OAAO,CACb,KAAK,CAAC;gBACJ,SAAS,EAAE,IAAI,IAAI,EAAE,CAAC,WAAW,EAAE;gBACnC,SAAS,EAAE,IAAI,CAAC,SAAS;gBACzB,aAAa,EAAE,IAAI,CAAC,aAAa;gBACjC,MAAM;gBACN,QAAQ;gBACR,OAAO,EAAE,IAAI,CAAC,GAAG,EAAE,GAAG,KAAK;gBAC3B,OAAO;aACR,CAAC,CACH,CACF,IAAI,EAAE,CACR,CAAC;YAEF,MAAM,aAAa,GAA6B;gBAC9C,GAAG,QAAQ;gBACX,UAAU,EAAE,QAAQ,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,IAAI,CAAC,SAAS,CAAC;gBACvD,MAAM,EAAE,KAAK;gBACb,IAAI,EAAE,aAAa,CAAC,EAAE,KAAK,EAAE,MAAM,CAAC,KAAK,EAAE,MAAM,EAAE,QAAQ,CAAC,KAAK,EAAE,CAAC;aACrE,CAAC;YAEF,OAAO,aAAa,CAAC;QACvB,CAAC,CACF,CAAC;IACJ,CAAC;IAES,KAAK,CAAC,QAAQ,CACtB,MAAoD,EACpD,OAAkB;QAElB,MAAM,KAAK,GAAG,IAAI,CAAC,GAAG,EAAE,CAAC;QACzB,oCAAoC;QACpC,MAAM,OAAO,GAAG,WAAW,CAAC;YAC1B,KAAK,EAAE,MAAM,CAAC,KAAK;YACnB,SAAS,EAAE,IAAI,CAAC,SAAS;YACzB,OAAO,EAAE,IAAI,CAAC,MAAM,CAAC,KAAK;SAC3B,CAAC,CAAC;QAEH,MAAM,aAAa,GAAG,SAAS,CAAC,IAAI,CAAC,OAAO,EAAE,OAAO,CAAc,CAAC;QAEpE,gEAAgE;QAChE,MAAM,gBAAgB,GAAG,MAAM,IAAI,CACjC,OAAO,CAAC,GAAG,CAAC,CAAC,KAAK,EAAE,EAAE,CAAC,KAAK,CAAC,GAAG,CAAC,CAAC,KAAK,EAAE,EAAE,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC,EACxD,KAAK,EAAE,KAAe,EAAE,EAAE;YACxB,MAAM,QAAQ,GAAG,MAAM,IAAI,CAAC,cAAc,CACxC;gBACE,KAAK,EAAE,KAAK;gBACZ,KAAK,EAAE,IAAI,CAAC,MAAM,CAAC,KAAK;aACzB,EACD,aAAa,CACd,CAAC;YACF,OAAO,QAAQ,CAAC;QAClB,CAAC,EACD;YACE,WAAW,EACT,IAAI,CAAC,MAAM,CAAC,QAAQ,EAAE,qBAAqB;gBAC3C,QAAQ,CAAC,qBAAqB;SACjC,CACF,CAAC;QAEF,wDAAwD;QACxD,MAAM,cAAc,GAAG,gBAAgB,CAAC,GAAG,CAAC,CAAC,KAAK,EAAE,EAAE,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC,IAAI,EAAE,CAAC;QAE1E,qCAAqC;QACrC,MAAM,WAAW,GAAG,OAAO,CAAC,MAAM,CAAC,CAAC,GAAG,EAAE,IAAI,EAAE,EAAE;YAC/C,OAAO,GAAG,GAAG,IAAI,CAAC,MAAM,CAAC,CAAC,GAAG,EAAE,IAAI,EAAE,EAAE,CAAC,GAAG,GAAG,IAAI,CAAC,UAAU,EAAE,CAAC,CAAC,CAAC;QACpE,CAAC,EAAE,CAAC,CAAC,CAAC;QAEN,MAAM,EAAE,IAAI,EAAE,CAAC,EAAE,GAAG,UAAU,EAAE,GAAG,gBAAgB,CAAC,CAAC,CAAC,CAAC;QACvD,MAAM,KAAK,GAAG,EAAE,aAAa,EAAE,WAAW,EAAE,YAAY,EAAE,WAAW,EAAE,CAAC;QACxE,MAAM,aAAa,GAA6B;YAC9C,GAAG,UAAU;YACb,KAAK;YACL,IAAI,EAAE,cAAc;YACpB,UAAU,EAAE,gBAAgB,CAAC,GAAG,CAAC,CAAC,KAAK,EAAE,EAAE,CAAC,KAAK,CAAC,UAAU,CAAC,CAAC,IAAI,EAAE;YACpE,MAAM,EAAE,KAAK;YACb,IAAI,EAAE,aAAa,CAAC,EAAE,KAAK,EAAE,MAAM,CAAC,KAAK,EAAE,MAAM,EAAE,KAAK,EAAE,CAAC;YAC3D,OAAO,EAAE,IAAI,CAAC,GAAG,EAAE,GAAG,KAAK;SAC5B,CAAC;QAEF,OAAO,aAAa,CAAC;IACvB,CAAC;IAED,gEAAgE;IAChE,KAAK,CAAC,IAAyB;QAC7B,MAAM,EAAE,QAAQ,EAAE,KAAK,EAAE,MAAM,EAAE,OAAO,EAAE,KAAK,EAAE,MAAM,EAAE,MAAM,EAAE,GAC/D,IAAI,IAAI,EAAE,CAAC;QAEb,aAAa;QACb,OAAO,IAAI,cAAc,CAAC;YACxB,QAAQ,EAAE,QAAQ,IAAI,IAAI,CAAC,QAAQ;YACnC,KAAK,EAAE,KAAK,IAAI,IAAI,CAAC,KAAK;YAC1B,MAAM,EAAE,MAAM,IAAI,IAAI,CAAC,MAAM;YAC7B,OAAO,EAAE,IAAI,CAAC,YAAY,CAAC,IAAI,CAAC,OAAO,EAAE,OAAO,CAAC;YACjD,KAAK,EAAE,KAAK,IAAI,IAAI,CAAC,KAAK;YAC1B,MAAM,EAAE,IAAI,CAAC,WAAW,CAAC,IAAI,CAAC,MAAM,EAAE,MAAM,IAAI,EAAE,CAAC;YACnD,MAAM,EAAE,IAAI,CAAC,WAAW,CAAC,IAAI,CAAC,MAAM,EAAE,MAAM,IAAI,EAAE,CAAC;SACpD,CAAC,CAAC;IACL,CAAC;CACF;AAID;;;GAGG;AACH,SAAS,WAAW,CAAC,IAIpB;IACC,MAAM,EAAE,KAAK,EAAE,MAAM,EAAE,SAAS,EAAE,OAAO,EAAE,GAAG,IAAI,CAAC;IACnD,MAAM,EACJ,iBAAiB,GAAG,QAAQ,CAAC,iBAAiB,EAC9C,YAAY,GAAG,QAAQ,CAAC,YAAY,GACrC,GAAG,OAAO,IAAI,EAAE,CAAC;IAElB,iCAAiC;IACjC,MAAM,OAAO,GAAiB,EAAE,CAAC;IACjC,IAAI,YAAY,GAAe,EAAE,CAAC;IAClC,IAAI,kBAAkB,GAAG,CAAC,CAAC;IAE3B,KAAK,IAAI,KAAK,IAAI,MAAM,EAAE,CAAC;QACzB,MAAM,UAAU,GAAG,SAAS,CAAC,WAAW,CAAC,KAAK,CAAC,CAAC;QAEhD,iEAAiE;QACjE,IAAI,UAAU,GAAG,iBAAiB,EAAE,CAAC;YACnC,MAAM,IAAI,KAAK,CACb,uCAAuC,UAAU,MAAM,iBAAiB,EAAE,CAC3E,CAAC;QACJ,CAAC;QAED,mCAAmC;QACnC,IAAI,YAAY,CAAC,MAAM,KAAK,CAAC,EAAE,CAAC;YAC9B,YAAY,CAAC,IAAI,CAAC,EAAE,IAAI,EAAE,KAAK,EAAE,UAAU,EAAE,CAAC,CAAC;YAC/C,kBAAkB,IAAI,UAAU,CAAC;YACjC,SAAS;QACX,CAAC;QAED,6EAA6E;QAC7E,MAAM,cAAc,GAAG,kBAAkB,GAAG,UAAU,GAAG,iBAAiB,CAAC;QAC3E,MAAM,aAAa,GAAG,YAAY,CAAC,MAAM,GAAG,CAAC,GAAG,YAAY,CAAC;QAE7D,sDAAsD;QACtD,IAAI,cAAc,IAAI,aAAa,EAAE,CAAC;YACpC,OAAO,CAAC,IAAI,CAAC,YAAY,CAAC,CAAC;YAC3B,YAAY,GAAG,CAAC,EAAE,IAAI,EAAE,KAAK,EAAE,UAAU,EAAE,CAAC,CAAC;YAC7C,kBAAkB,GAAG,UAAU,CAAC;YAChC,SAAS;QACX,CAAC;QAED,qCAAqC;QACrC,YAAY,CAAC,IAAI,CAAC,EAAE,IAAI,EAAE,KAAK,EAAE,UAAU,EAAE,CAAC,CAAC;QAC/C,kBAAkB,IAAI,UAAU,CAAC;IACnC,CAAC;IAED,qBAAqB;IACrB,IAAI,YAAY,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC;QAC5B,OAAO,CAAC,IAAI,CAAC,YAAY,CAAC,CAAC;IAC7B,CAAC;IAED,OAAO,OAAO,CAAC;AACjB,CAAC","sourcesContent":["import pThrottle from 'p-throttle';\nimport pMap from 'p-map';\nimport type { SetOptional } from 'type-fest';\nimport type { ModelArgs } from './model.js';\nimport type { Model } from './types.js';\nimport { calculateCost } from './utils/calculate-cost.js';\nimport { createOpenAIClient } from './clients/openai.js';\nimport { AbstractModel } from './model.js';\nimport { deepMerge } from '../utils/helpers.js';\n\nexport type EmbeddingModelArgs = SetOptional<\n  ModelArgs<\n    Model.Embedding.Client,\n    Model.Embedding.Config,\n    Model.Embedding.Run,\n    Model.Embedding.Response\n  >,\n  'client' | 'params'\n>;\n\ntype BulkEmbedder = (\n  params: Model.Embedding.Run & Model.Embedding.Config,\n  context: Model.Ctx\n) => Promise<Model.Embedding.Response>;\n\nconst DEFAULTS = {\n  /** The interval that the OpenAI API rate limit resets */\n  throttleInterval: 1000 * 60, // 1 minute\n  maxBatchSize: 10,\n  maxTokensPerBatch: 20000,\n  maxConcurrentRequests: 1,\n  maxRequestsPerMin: 3500,\n  model: 'text-embedding-ada-002',\n} as const;\n\nexport class EmbeddingModel extends AbstractModel<\n  Model.Embedding.Client,\n  Model.Embedding.Config,\n  Model.Embedding.Run,\n  Model.Embedding.Response,\n  Model.Embedding.ApiResponse\n> {\n  modelType = 'embedding' as const;\n  modelProvider = 'openai' as const;\n  throttledModel: BulkEmbedder;\n\n  /** Doesn't accept OpenAIClient because retry needs to be handled at the model level. */\n  constructor(args?: EmbeddingModelArgs) {\n    let { client, params, ...rest } = args || {};\n    client = client || createOpenAIClient();\n    params = params || { model: DEFAULTS.model };\n    super({ client, params, ...rest });\n    const interval = DEFAULTS.throttleInterval;\n    const limit =\n      this.params.throttle?.maxRequestsPerMin || DEFAULTS.maxRequestsPerMin;\n\n    // Create the throttled function\n    this.throttledModel = pThrottle({ limit, interval })(\n      async (\n        params: Model.Embedding.Run & Model.Embedding.Config,\n        context: Model.Ctx\n      ) => {\n        const start = Date.now();\n\n        // Make the request to OpenAI API to generate embeddings\n        const response = await this.client.createEmbeddings({\n          model: params.model,\n          input: params.input,\n        });\n\n        await Promise.allSettled(\n          this.events?.onApiResponse?.map((event) =>\n            Promise.resolve(\n              event({\n                timestamp: new Date().toISOString(),\n                modelType: this.modelType,\n                modelProvider: this.modelProvider,\n                params,\n                response,\n                latency: Date.now() - start,\n                context,\n              })\n            )\n          ) ?? []\n        );\n\n        const modelResponse: Model.Embedding.Response = {\n          ...response,\n          embeddings: response.data.map((item) => item.embedding),\n          cached: false,\n          cost: calculateCost({ model: params.model, tokens: response.usage }),\n        };\n\n        return modelResponse;\n      }\n    );\n  }\n\n  protected async runModel(\n    params: Model.Embedding.Run & Model.Embedding.Config,\n    context: Model.Ctx\n  ): Promise<Model.Embedding.Response> {\n    const start = Date.now();\n    // Batch the inputs for the requests\n    const batches = batchInputs({\n      input: params.input,\n      tokenizer: this.tokenizer,\n      options: this.params.batch,\n    });\n\n    const mergedContext = deepMerge(this.context, context) as Model.Ctx;\n\n    // Make the requests in parallel, respecting concurrency setting\n    const embeddingBatches = await pMap(\n      batches.map((batch) => batch.map((input) => input.text)),\n      async (batch: string[]) => {\n        const response = await this.throttledModel(\n          {\n            input: batch,\n            model: this.params.model,\n          },\n          mergedContext\n        );\n        return response;\n      },\n      {\n        concurrency:\n          this.params.throttle?.maxConcurrentRequests ||\n          DEFAULTS.maxConcurrentRequests,\n      }\n    );\n\n    // Flatten the batches of embeddings into a single array\n    const embeddingsObjs = embeddingBatches.map((batch) => batch.data).flat();\n\n    // Add up the tokens from the batches\n    const totalTokens = batches.reduce((acc, curr) => {\n      return acc + curr.reduce((acc, curr) => acc + curr.tokenCount, 0);\n    }, 0);\n\n    const { data: _, ...firstBatch } = embeddingBatches[0];\n    const usage = { prompt_tokens: totalTokens, total_tokens: totalTokens };\n    const modelResponse: Model.Embedding.Response = {\n      ...firstBatch,\n      usage,\n      data: embeddingsObjs,\n      embeddings: embeddingBatches.map((batch) => batch.embeddings).flat(),\n      cached: false,\n      cost: calculateCost({ model: params.model, tokens: usage }),\n      latency: Date.now() - start,\n    };\n\n    return modelResponse;\n  }\n\n  /** Clone the model and merge/orverride the given properties. */\n  clone(args?: EmbeddingModelArgs): this {\n    const { cacheKey, cache, client, context, debug, params, events } =\n      args ?? {};\n\n    // @ts-ignore\n    return new EmbeddingModel({\n      cacheKey: cacheKey ?? this.cacheKey,\n      cache: cache ?? this.cache,\n      client: client ?? this.client,\n      context: this.mergeContext(this.context, context),\n      debug: debug ?? this.debug,\n      params: this.mergeParams(this.params, params ?? {}),\n      events: this.mergeEvents(this.events, events || {}),\n    });\n  }\n}\n\ntype InputBatch = { text: string; tokenCount: number }[];\n\n/**\n * Split text inputs into batches, respecting token limits and batch size.\n * @throws {Error} If an input exceeds the max tokens per batch.\n */\nfunction batchInputs(args: {\n  input: string[];\n  tokenizer: Model.ITokenizer;\n  options?: Partial<Model.Embedding.BatchOptions>;\n}): InputBatch[] {\n  const { input: inputs, tokenizer, options } = args;\n  const {\n    maxTokensPerBatch = DEFAULTS.maxTokensPerBatch,\n    maxBatchSize = DEFAULTS.maxBatchSize,\n  } = options || {};\n\n  // State for constructing batches\n  const batches: InputBatch[] = [];\n  let currentBatch: InputBatch = [];\n  let currentBatchTokens = 0;\n\n  for (let input of inputs) {\n    const tokenCount = tokenizer.countTokens(input);\n\n    // Ensure that the input does not exceed the max tokens per batch\n    if (tokenCount > maxTokensPerBatch) {\n      throw new Error(\n        `Input exceeds max tokens per batch: ${tokenCount} > ${maxTokensPerBatch}`\n      );\n    }\n\n    // Add the first input to the batch\n    if (currentBatch.length === 0) {\n      currentBatch.push({ text: input, tokenCount });\n      currentBatchTokens += tokenCount;\n      continue;\n    }\n\n    // Check if the input would exceed the max tokens per batch or max batch size\n    const overTokenLimit = currentBatchTokens + tokenCount > maxTokensPerBatch;\n    const overBatchSize = currentBatch.length + 1 > maxBatchSize;\n\n    // Start a new batch if either limit would be exceeded\n    if (overTokenLimit || overBatchSize) {\n      batches.push(currentBatch);\n      currentBatch = [{ text: input, tokenCount }];\n      currentBatchTokens = tokenCount;\n      continue;\n    }\n\n    // Add the input to the current batch\n    currentBatch.push({ text: input, tokenCount });\n    currentBatchTokens += tokenCount;\n  }\n\n  // Add the last batch\n  if (currentBatch.length > 0) {\n    batches.push(currentBatch);\n  }\n\n  return batches;\n}\n"]}