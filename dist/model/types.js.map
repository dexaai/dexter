{"version":3,"file":"types.js","sourceRoot":"","sources":["../../src/model/types.ts"],"names":[],"mappings":"","sourcesContent":["import type {\n  ChatMessage,\n  ChatParams,\n  ChatResponse,\n  ChatStreamResponse,\n  CompletionParams,\n  CompletionResponse,\n  EmbeddingParams,\n  EmbeddingResponse,\n  OpenAIClient,\n} from 'openai-fetch';\nimport type { AbstractModel } from './model.js';\nimport type { ChatModel } from './chat.js';\nimport type { CompletionModel } from './completion.js';\nimport type { EmbeddingModel } from './embedding.js';\nimport type { SparseVectorModel } from './sparse-vector.js';\n\ntype InnerType<T> = T extends ReadableStream<infer U> ? U : never;\n\n/**\n * Generic Model extended by provider specific implementations.\n */\nexport namespace Model {\n  /**\n   * Base model\n   */\n  export namespace Base {\n    /** Client for making API calls. Extended by specific model clients. */\n    export type Client = any;\n    export interface Config {\n      model: string;\n    }\n    export interface Run {}\n    export interface Params extends Config, Run {}\n    export interface Response {\n      cached: boolean;\n      latency?: number;\n      cost?: number;\n    }\n    export type Model = AbstractModel<Client, Config, Run, Response, any>;\n  }\n\n  /**\n   * Chat Model\n   */\n  export namespace Chat {\n    export type Client = {\n      createChatCompletion: OpenAIClient['createChatCompletion'];\n      streamChatCompletion: OpenAIClient['streamChatCompletion'];\n    };\n    export interface Run extends Base.Run {\n      messages: Model.Message[];\n    }\n    export interface Config extends Base.Config {\n      /** Handle new chunk from streaming requests. */\n      handleUpdate?: (chunk: string) => void;\n      frequency_penalty?: ChatParams['frequency_penalty'];\n      function_call?: ChatParams['function_call'];\n      functions?: ChatParams['functions'];\n      logit_bias?: ChatParams['logit_bias'];\n      max_tokens?: ChatParams['max_tokens'];\n      model: ChatParams['model'];\n      presence_penalty?: ChatParams['presence_penalty'];\n      response_format?: ChatParams['response_format'];\n      seed?: ChatParams['seed'];\n      stop?: ChatParams['stop'];\n      temperature?: ChatParams['temperature'];\n      tools?: ChatParams['tools'];\n      tool_choice?: ChatParams['tool_choice'];\n      top_p?: ChatParams['top_p'];\n    }\n    export interface Response extends Base.Response, ChatResponse {\n      message: ChatMessage;\n    }\n    /** Streaming response from the OpenAI API. */\n    type StreamResponse = ChatStreamResponse;\n    /** A chunk recieved from a streaming response */\n    export type CompletionChunk = InnerType<StreamResponse>;\n    export type ApiResponse = ChatResponse;\n    export type Model = ChatModel;\n  }\n\n  /**\n   * Completion model\n   */\n  export namespace Completion {\n    export type Client = {\n      createCompletions: OpenAIClient['createCompletions'];\n    };\n    export interface Run extends Base.Run {\n      prompt:\n        | string\n        | Array<string>\n        | Array<number>\n        | Array<Array<number>>\n        | null;\n    }\n    export interface Config\n      extends Base.Config,\n        Omit<CompletionParams, 'prompt' | 'user'> {\n      model: CompletionParams['model'];\n    }\n    export interface Response extends Base.Response, CompletionResponse {\n      completion: string;\n    }\n    export type ApiResponse = CompletionResponse;\n    export type Model = CompletionModel;\n  }\n\n  /** Generic metadata object. */\n  export type Ctx = { [key: string]: any };\n\n  /**\n   * Embedding Model\n   */\n  export namespace Embedding {\n    export type Client = {\n      createEmbeddings: OpenAIClient['createEmbeddings'];\n    };\n    export interface Run extends Base.Run {\n      input: string[];\n    }\n    /** API request batching options */\n    export interface BatchOptions {\n      maxTokensPerBatch: number;\n      maxBatchSize: number;\n    }\n    /** API request throttling options */\n    interface ThrottleOptions {\n      maxRequestsPerMin: number;\n      maxConcurrentRequests: number;\n    }\n    export interface Config\n      extends Base.Config,\n        Omit<EmbeddingParams, 'input' | 'user'> {\n      model: EmbeddingParams['model'];\n      batch?: Partial<BatchOptions>;\n      throttle?: Partial<ThrottleOptions>;\n    }\n    export interface Response extends Base.Response, EmbeddingResponse {\n      embeddings: number[][];\n    }\n    export type ApiResponse = EmbeddingResponse;\n    export type Model = EmbeddingModel;\n  }\n\n  /**\n   * Event handlers for logging and debugging\n   */\n  export interface Events<\n    MParams extends Base.Params,\n    MResponse extends Base.Response,\n    AResponse extends any = any\n  > {\n    onStart?: ((event: {\n      timestamp: string;\n      modelType: Type;\n      modelProvider: Provider;\n      params: MParams;\n      context: Ctx;\n    }) => void | Promise<void>)[];\n    onApiResponse?: ((event: {\n      timestamp: string;\n      modelType: Type;\n      modelProvider: Provider;\n      params: MParams;\n      response: AResponse;\n      latency: number;\n      context: Ctx;\n    }) => void | Promise<void>)[];\n    onComplete?: ((event: {\n      timestamp: string;\n      modelType: Type;\n      modelProvider: Provider;\n      params: MParams;\n      response: MResponse;\n      context: Ctx;\n      cached: boolean;\n    }) => void | Promise<void>)[];\n    onError?: ((event: {\n      timestamp: string;\n      modelType: Type;\n      modelProvider: Provider;\n      params: MParams;\n      error: unknown;\n      context: Ctx;\n    }) => void | Promise<void>)[];\n  }\n\n  /**\n   * Generic interface for a model tokenizer\n   */\n  export interface ITokenizer {\n    /** Tokenize a string into an array of integer tokens */\n    encode(text: string): Uint32Array;\n    /** Decode an array of integer tokens into a string */\n    decode(tokens: number[] | Uint32Array): string;\n    /**\n     * Count the number of tokens in a string or ChatMessage(s).\n     * A single ChatMessage is counted as a completion and an array as a prompt.\n     * Strings are counted as is.\n     */\n    countTokens(input?: string | ChatMessage | ChatMessage[]): number;\n    /** Truncate a string to a maximum number of tokens */\n    truncate(args: {\n      /** Text to truncate */\n      text: string;\n      /** Maximum number of tokens to keep (inclusive) */\n      max: number;\n      /** Truncate from the start or end of the text */\n      from?: 'start' | 'end';\n    }): string;\n  }\n\n  /** Primary message type for chat models */\n  export type Message = ChatMessage;\n\n  /** The provider of the model (eg: OpenAI) */\n  export type Provider = (string & {}) | 'openai' | 'custom';\n\n  /**\n   * Sparse vector model (SPLADE)\n   */\n  export namespace SparseVector {\n    export type Client = {\n      createSparseVector: (\n        params: {\n          input: string;\n          model: string;\n        },\n        serviceUrl: string\n      ) => Promise<SparseVector.Vector>;\n    };\n    /** Sparse vector from SPLADE models. */\n    export type Vector = {\n      indices: number[];\n      values: number[];\n    };\n    export interface Run extends Model.Base.Run {\n      input: string[];\n    }\n    export interface Config extends Model.Base.Config {\n      concurrency?: number;\n      throttleLimit?: number;\n      throttleInterval?: number;\n    }\n    export interface Response extends Model.Base.Response {\n      vectors: Vector[];\n    }\n    export type Model = SparseVectorModel;\n  }\n\n  /** The type of data returned by the model */\n  export type Type =\n    | (string & {})\n    | 'base'\n    | 'completion'\n    | 'chat'\n    | 'embedding'\n    | 'sparse-vector';\n}\n"]}